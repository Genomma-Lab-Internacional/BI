{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import jupyternotify\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import requests\n",
    "import pyodbc\n",
    "import json\n",
    "import os\n",
    "from mysql.connector import errorcode\n",
    "from mysql.connector import Error\n",
    "from urllib.request import urlopen\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "from time import sleep\n",
    "\n",
    "ip = get_ipython()\n",
    "ip.register_magics(jupyternotify.JupyterNotifyMagics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'Ecuador'\n",
    "path = '../../ISVParams/'\n",
    "\n",
    "# jsons\n",
    "for json_file in [file for file in os.listdir(path) if file.endswith('.json')]:  \n",
    "    with open(path + json_file, encoding='utf8') as f:\n",
    "        globals()[json_file.split('.')[0].split('_')[1]] = json.load(f)\n",
    "\n",
    "# queries\n",
    "for sql_file in [file for file in os.listdir(path) if file.endswith('.sql')]:\n",
    "    globals()[sql_file.split('.')[0]] = open(path + sql_file, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API parameters\n",
    "## Sales\n",
    "sales['headers_sales']['Authorization'] = 'ISVToken ' + tokens[country]\n",
    "body_sales = sales['body_sales']\n",
    "body_sales['views'] = sales['views'][country]\n",
    "body_sales['hierarchy'] = sales['hierarchy_sales'][country]\n",
    "body_sales['view_type'] = 'semana'\n",
    "\n",
    "## Stock\n",
    "url_stock = stocks['url_stock']\n",
    "headers_stock = stocks['headers_stock']\n",
    "headers_stock['Authorization'] = 'ISVToken ' + tokens[country]\n",
    "hierarchy_stock = stocks['hierarchy_stock']\n",
    "body_stock = stocks['body_stock']\n",
    "body_stock['hierarchy'] = hierarchy_stock\n",
    "\n",
    "## Store\n",
    "url_stores = stores['url_stores']\n",
    "headers_stores = stores['headers_stores']\n",
    "headers_stores['Authorization'] = 'ISVToken ' + tokens[country]\n",
    "\n",
    "# Conection to SQL Server\n",
    "conn1 = pyodbc.connect('Driver={SQL Server};Server=' + serversdbs['server'] + ';Database=' + serversdbs['database1'] + ';Trusted_Connection=yes;')\n",
    "conn2 = pyodbc.connect('Driver={SQL Server};Server=' + serversdbs['server'] + ';Database=' + serversdbs['database2'] + ';Trusted_Connection=yes;')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7304, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TmpID\n",
    "df_tmpid = pd.read_sql(query_dates.read(), conn1)\n",
    "df_tmpid['TmpFecha'] = df_tmpid['TmpFecha'].astype(str).copy()\n",
    "\n",
    "# ProPstID\n",
    "df_ppst = pd.read_sql(query_ppst.read().replace(\"''\", \"'\" + country + \"'\"), conn2)\n",
    "df_ppst['ProPstCodBarras'] = df_ppst['ProPstCodBarras'].astype(str).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_format = lambda number: '0' + str(number) if (number < 10) else str(number)\n",
    "\n",
    "clean_date = lambda str_date: datetime(int(str_date.split('-')[2]), int(str_date.split('-')[1]), int(str_date.split('-')[0]))\n",
    "\n",
    "clean_numbers = lambda str_numb: float(str(str_numb).replace(',',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_file(response):\n",
    "    file = urlopen(response.json()['download_url'])\n",
    "    zip_file = ZipFile(BytesIO(file.read()))\n",
    "    df = pd.read_csv(zip_file.open(zip_file.namelist()[0]), encoding='latin-1', sep=';')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_status(status, date):\n",
    "    if status != 200:\n",
    "        return (date, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sales(num_week, url, header, body):\n",
    "    body[\"dates\"] = [\"2020-W\" + weekly_format(num_week)]\n",
    "    resp_sales = requests.post(url, data=json.dumps(body), headers=header)\n",
    "    if resp_sales.status_code != 200:\n",
    "        print(\"Oh, oh, adventurous, problems  in week \" + str(num_week) + \" :S\")\n",
    "        pass\n",
    "    else:\n",
    "        df = unpack_data(resp_sales)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sales(data):    \n",
    "    data['Cód. Cadena'] = data['Cód. Cadena'].astype(str).copy()\n",
    "    data['EAN'] = data['EAN'].astype(str).copy()    \n",
    "    for col in ['Unidades', 'Costos B2B']:\n",
    "        data[col] = data[col].map(clean_numbers).copy()\n",
    "    data['ID_SaSt'] = data['EAN'].astype(str) + data['Cód. Cadena'].astype(str) + data['Cadena'] + data['Local']  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stores(url, header):\n",
    "    resp_stores = requests.get(url, headers=header)\n",
    "    if resp_stores.status_code != 200:\n",
    "        print(\"Oh, oh, problems body\")\n",
    "    else:\n",
    "        df = pd.read_excel(resp_stores.json()['url'], sep='\\t')\n",
    "        ind_min = df[df.iloc[:,7].notnull()].index.min()\n",
    "        cols = df.iloc[9,:].tolist()\n",
    "        data = df.loc[ind_min + 2:].copy()\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "        data.rename(dict(zip(df.columns.tolist(), cols)), inplace=True, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_stock(dates, chain, url, header, body):\n",
    "    df_stock = pd.DataFrame()\n",
    "    hierarchy_stock['Cadena'] = chain\n",
    "    for date in dates:\n",
    "        body[\"dates\"] = [date.strftime('%Y-%m-%d')]\n",
    "        resp_stock = requests.post(url, data=json.dumps(body), headers=header)\n",
    "        if resp_stock.status_code != 200:\n",
    "            print(\"Oh, oh, adventurous, problems in \" + date.strftime('%d/%m/%Y') + ' :S')\n",
    "            pass\n",
    "        else:\n",
    "            resp_page = urlopen(resp_stock.json()['download_url'])\n",
    "            zip_file = ZipFile(BytesIO(resp_page.read()))\n",
    "            df = pd.read_csv(zip_file.open(zip_file.namelist()[0]), encoding='latin-1', sep=';')\n",
    "            df_stock = pd.concat([df_stock, df], axis=0)\n",
    "    df_stock.reset_index(drop=True, inplace=True)\n",
    "    return df_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_stock(dates, chains):\n",
    "    df = pd.DataFrame()\n",
    "    for date in dates:\n",
    "        df = pd.concat([df, download_stock(dates=[date], chain=[], url=url_stock, header=headers_stock, body=body_stock)], axis=0)\n",
    "        for chain in chains:\n",
    "            chain_stock = pd.DataFrame()\n",
    "            days = 0\n",
    "            if chain not in df['Cadena'].unique():                \n",
    "                while chain_stock.shape[0] == 0:\n",
    "                    days += 1            \n",
    "                    chain_stock = download_stock(dates=[date - timedelta(days=days)], chain=[chain], url=url_stock, header=headers_stock, body=body_stock)\n",
    "                    sleep(17)\n",
    "                    if days > 5:\n",
    "                        break\n",
    "            try:\n",
    "                df = pd.concat([df, chain_stock[chain_stock['Cadena'] == chain]], axis=0)\n",
    "            except:\n",
    "                pass\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stock(data):\n",
    "    data_clean = data[['Fechas', 'EAN', 'Cadena', 'Sub Cadena', 'Local', 'Stock Locales en Unidades', 'Stock CD en Unidades', 'Stock en Tránsito en Unidades', 'Stock Total en Unidades']].copy()\n",
    "    for col in ['Stock Locales en Unidades', 'Stock CD en Unidades', 'Stock en Tránsito en Unidades', 'Stock Total en Unidades']:\n",
    "        data_clean[col] = data_clean[col].map(clean_numbers)\n",
    "    data_clean['ID_SaSt'] = data['EAN'].astype(str) + data['Cód. Cadena'].astype(str) + data['Cadena'] + data['Local']\n",
    "    data_clean['ID_StSt'] = data['Cadena'] + data['Sub Cadena'] + data['Local']\n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill = lambda col1, col2: col1 if pd.isnull(col2) else (col2 if pd.isnull(col1) else col1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relate_dates(df1, df2):\n",
    "    d = {}\n",
    "    for date1 in df1:\n",
    "        for date2 in df2:\n",
    "            d11 = clean_date(date1.split(' ')[2])\n",
    "            d12 = clean_date(date1.split(' ')[-1])\n",
    "            d2 = clean_date(date2)\n",
    "            if d11 <= d2 <= d12:\n",
    "                d[d2.strftime('%d-%m-%Y')] = d12.strftime('%d-%m-%Y')\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_data(sales, stock, stores, df_ppst):\n",
    "        join = pd.merge(sales, stock, on='ID_SaSt', how='outer')\n",
    "        join['Cadena'] = join['Cadena_y'].combine(join['Cadena_x'], fill)\n",
    "        join['Sub Cadena'] = join['Sub Cadena_y'].combine(join['Sub Cadena_x'], fill)\n",
    "        join['Local'] = join['Local_y'].combine(join['Local_x'], fill)\n",
    "        join['EAN'] = join['EAN_y'].combine(join['EAN_x'], fill)\n",
    "        join['Fecha'] = join['Fecha_y'].combine(join['Fecha_x'], fill)\n",
    "        join.drop(['Cadena_x', 'Cadena_y', 'Sub Cadena_x', 'Sub Cadena_y', 'Local_x', 'Local_y', 'EAN_x', 'EAN_y', 'Fecha_x', 'Fecha_y', 'ID_SaSt', 'ID_StSt'], axis=1, inplace=True)\n",
    "        join.fillna(0, inplace=True)\n",
    "        join2 = pd.merge(join, stores, on='Local', how='left')\n",
    "        join2.rename({'EAN':'ProPstCodBarras'}, axis=1, inplace=True)\n",
    "        join2 = join2[join2['ProPstCodBarras'] != 'No Definido'].copy()\n",
    "        join2['ProPstCodBarras'] = join2['ProPstCodBarras'].astype('int64').astype(str)\n",
    "        join3 = pd.merge(join2, df_ppst, on='ProPstCodBarras', how='left')\n",
    "        return join3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status(df1, df2, df3):\n",
    "    cols = ['Unidades', 'Costos B2B', 'Stock Locales en Unidades',\n",
    "           'Stock CD en Unidades', 'Stock en Tránsito en Unidades',\n",
    "           'Stock Total en Unidades']\n",
    "\n",
    "    for col in cols:\n",
    "        try:\n",
    "            sum_equals = (df1[col].sum() == df2[col].sum())\n",
    "            if sum_equals == False:\n",
    "                diff = (df1[col].sum() - df2[col].sum())\n",
    "        except:\n",
    "            sum_equals = df1[col].sum() == df3[col].sum()\n",
    "            if sum_equals == False:\n",
    "                diff = (df1[col].sum() - df3[col].sum())\n",
    "        if sum_equals == False:\n",
    "            print(sum_equals, \" - \", col, '  -  Difference:', diff)\n",
    "        else:\n",
    "            print(sum_equals, \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí es donde hace casi toda la magia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%notify -m \"¡Descarga de ECUADOR lista!\"\n",
    "%%time\n",
    "# Semana ISV = Semana Genomma - 1\n",
    "final = pd.DataFrame()\n",
    "df_sales = {}\n",
    "data_sales = {}\n",
    "df_stock = {}\n",
    "data_stock = {}\n",
    "#df_stores = pd.read_excel('../../1Data/2Catalogue/SucID_41_43.xlsx')\n",
    "df_stores = download_stores(url_stores, headers_stores)\n",
    "stores = df_stores[['Local', 'Suc. ID']].copy()\n",
    "weeks = [datetime.today().isocalendar()[1] - i for i in range(3, 1, -1)]\n",
    "for week in weeks:\n",
    "    ## Download the sales data\n",
    "    df_sales[str(week)] = download_sales(week, url=sales['url_sales'], body=body_sales, header=sales['headers_sales'])\n",
    "    ## Clean it\n",
    "    data_sales[str(week)] = clean_sales(df_sales[str(week)])\n",
    "    ## Download the stock data\n",
    "    df_stock[str(week)] = iterate_stock([clean_date(df_sales[str(week)]['Semanas'].unique()[0][-10:])], df_sales[str(week)]['Cadena'].unique().tolist())       \n",
    "    ## Clean it\n",
    "    data_stock[str(week)] = clean_stock(df_stock[str(week)])\n",
    "    ## Verify the stock data\n",
    "    print(\"Total de stock a la semana \" + str(week + 1) + \":\", \"\\n\")\n",
    "    print(pd.pivot_table(data_stock[str(week)], index=['Cadena'], columns=['Fechas'], values=['Stock Locales en Unidades'], aggfunc='sum'))\n",
    "    ## Assign a date to sales data and stock data\n",
    "    dict_dates = relate_dates(data_sales[str(week)]['Semanas'].unique(), data_stock[str(week)]['Fechas'].unique())\n",
    "    ## Format the date to sales data\n",
    "    sellout = data_sales[str(week)][['Semanas', 'Cadena', 'Sub Cadena', 'Local','EAN','Unidades', 'Costos B2B', 'ID_SaSt']].copy()\n",
    "    sellout['Fecha'] = sellout['Semanas'].apply(lambda x: x[-10:]).copy()\n",
    "    sellout.drop(['Semanas'], axis=1, inplace=True)\n",
    "    ## Format the date to stock data\n",
    "    stock = data_stock[str(week)].copy()\n",
    "    stock['Fecha'] = data_stock[str(week)]['Fechas'].map(dict_dates)\n",
    "    stock.drop(['Fechas'], axis=1, inplace=True)\n",
    "    ## Merge sales, stock and stores data\n",
    "    all_data = join_data(sellout, stock, stores, df_ppst)\n",
    "    ## We verify some fields\n",
    "    print('\\n', 'Missings por columna:\\n', all_data.isnull().sum())\n",
    "    print('\\n', 'Códigos de Barra sin ProPstID  ', all_data[all_data['ProPstID'].isnull()]['ProPstCodBarras'].unique(), '\\n')\n",
    "    status(all_data, data_stock[str(week)], data_sales[str(week)])\n",
    "    ## We concant into a single variable\n",
    "    final = pd.concat([final, all_data], axis=0)\n",
    "    print('\\n\\n')\n",
    "    print('-----------------------------------------------------------------------')\n",
    "    print('\\n\\n')\n",
    "final.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usualmente cuando las sucursales (_Suc. ID_) no están en ISV, vienen como _\"No Definidas\"_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26913, 13)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: 34094 \n",
      " max: 146777\n"
     ]
    }
   ],
   "source": [
    "print('min:', final['Suc. ID'].astype('int64').min(), '\\n','max:', final['Suc. ID'].astype('int64').max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = {'Unidades':'SoutCantDesp',\n",
    "             'Stock Locales en Unidades':'SoutCantExist',\n",
    "             'Stock CD en Unidades':'SoutCantCedis',\n",
    "             'Stock en Tránsito en Unidades':'SoutCantTrans',\n",
    "             'Stock Total en Unidades':'SoutCantInv',\n",
    "             'Costos B2B':'SoutMontoDesp',\n",
    "             'Stock Locales en Precio Lista':'SoutMontoExist',\n",
    "             'Stock CD en Precio Lista':'SoutMontoCedis',\n",
    "             'Stock Total en Precio Lista':'SoutMontoInv',             \n",
    "             'EAN_x':'ProPstCodBarras',\n",
    "             'Suc. ID':'SucID',\n",
    "             'Pro. Pst. ID':'ProPstID'}\n",
    "\n",
    "final.rename(new_names, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['TmpFecha'] = final['Fecha'].map(clean_date).astype(str)\n",
    "\n",
    "final = pd.merge(final, df_tmpid, on='TmpFecha', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['SoutCantStaple'] = 0\n",
    "final['SoutMontoTrans'] = 0\n",
    "final['SoutMontoStaple'] = 0\n",
    "final['SoutMontoExist'] = 0\n",
    "final['SoutMontoCedis'] = 0\n",
    "final['SoutMontoTrans'] = 0\n",
    "final['SoutMontoStaple'] = 0\n",
    "final['SoutMontoInv'] = 0\n",
    "final['SoutMontoCteDesp'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dd52987f896e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ProPstID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SucID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TmpID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SoutCantDesp'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SoutCantExist'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SoutCantCedis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SoutCantTrans'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SoutCantStaple'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SoutCantInv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SoutMontoDesp'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SoutMontoExist'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SoutMontoCedis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SoutMontoTrans'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SoutMontoStaple'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SoutMontoInv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SoutMontoCteDesp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sum'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "X = pd.pivot_table(final, index=['ProPstID', 'SucID', 'TmpID'], values=['SoutCantDesp', 'SoutCantExist', 'SoutCantCedis', 'SoutCantTrans', 'SoutCantStaple', 'SoutCantInv', 'SoutMontoDesp', 'SoutMontoExist', 'SoutMontoCedis', 'SoutMontoTrans', 'SoutMontoStaple', 'SoutMontoInv', 'SoutMontoCteDesp'], aggfunc='sum').reset_index().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d19d37cee4f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'min:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ProPstID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'max:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ProPstID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "print('min:', X['ProPstID'].min(), '\\n','max:', X['ProPstID'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1d7c44ba1631>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'min:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SucID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'max:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SucID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "print('min:', X['SucID'].astype('int64').min(), '\\n','max:', X['SucID'].astype('int64').max())"
   ]
  },
  {
   "source": [
    "**Ojo, falta agregar la parte del reproceso, los layouts y los diferentes tipos de exportación**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda8f2870ef7b8b4a659a1cfb9279b52c39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}